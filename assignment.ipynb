{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MA5634/5663 - Fundamentals of Machine Learning\n",
    "\n",
    "## Assignment 2022/23 (First Sitting)\n",
    "\n",
    "This assignment carries 40% of the marks, with the remaining 60% coming from\n",
    "the unseen exam.\n",
    "\n",
    "You should refer to the project brief for further details relating to this\n",
    "assignment. \n",
    "\n",
    "The key instructions **YOU MUST** adhere to are as follows:\n",
    "\n",
    "- Enter your 7-digit student ID as the value of `ID` in the next cell. Ignore\n",
    "the backslash (if it is present) and any numbers that follow it.\n",
    "\n",
    "- All other prepopulated cells in this notebook should be left untouched.\n",
    "\n",
    "- It will be clear below which parts of this notebook contain code that\n",
    "produces data that you should use for your submission.\n",
    "\n",
    "- It will also be clear in which cells you should enter your submitted work.\n",
    "\n",
    "- Feel free to create more cells.\n",
    "\n",
    ">**REQUIREMENT:** This notebook will be assessed by executing it sequentially \n",
    "from the top down and in one session. It must run to completion and without\n",
    "error.\n",
    "\n",
    ">**NOTE: If you alter a variable's value in a cell low down the notebook and\n",
    "then execute a cell near the top that uses an unrelated variable with that\n",
    "same name, then the unwanted new value will be used. This can cause bugs.**\n",
    "\n",
    ">**REMEDY:** always execute your Jupyter notebook from the top down. An easy \n",
    "way to do this is to select _Run All Above_ from the *Cell* menu. This will \n",
    "ensure that code further down does not affect the present cell.\n",
    "\n",
    ">**NOTE:** you will be asked to discuss results in your report. Note that due\n",
    "to the randomization in the `sklearn` routines you may not always get the \n",
    "same results. For this reason it is acceptable to quote the results of a \n",
    "specific run in your report. However, make sure that these results are truly\n",
    "representative of the run and not just an outlier.\n",
    "\n",
    "\n",
    "## ENTER YOUR 7-digit STUDENT ID HERE ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 2309765  # replace this number with your 7-digit ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1\n",
    "\n",
    "You will create a $k$-NN binary classifier. \n",
    "\n",
    "You will be given a subset of the feature data from $569$ breast cancer test results. This subset is generated, along with personalized values of $k$ and $p$ (for the $p$-norm) for the $k$-NN method by the *untouchable* code below.\n",
    "Why *untouchable* - because **that code should not be altered in any way**.\n",
    "\n",
    "In this notebook for Task 1\n",
    "\n",
    "- Extract your data, check for invalid entries\n",
    "- Select a suitable train/test split fraction and gives the sizes of the resulting data sets.\n",
    "- Use the $k$-Nearest Neighbours method from `sklearn` to classify a breast cancer\n",
    "testing result as *benign* or *malignant*. \n",
    "- Plot the confusion matrix.\n",
    "- Give the accuracy score\n",
    "- Estimate the probability that the test is positive (malignant) given that the classifier predicts that it is negative (benign). Denote this as $\\mathrm{Prob}(P\\mid-)$.\n",
    "\n",
    "In you report for Task 1\n",
    "\n",
    "- Give a short overview of the $k$-NN method and explain its main features and hyperparameters.\n",
    "- Explain your choice of **train/test** split.\n",
    "- Explain how you calculated $\\mathrm{Prob}(P\\mid-)$.\n",
    "\n",
    "**- - DO NOT ALTER THE CONTENTS OF THE NEXT CELL(S) IN ANY WAY - -**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Original Features:  30\n",
      "Original data Frame shape: dfbigbc.shape =  (569, 31)\n",
      "We will work only with the features in columns: [0, 15, 27, 11, 29, 30]\n",
      "These features are ...\n",
      "['mean radius', 'compactness error', 'worst concave points', 'texture error', 'worst fractal dimension', 'target']\n",
      "Specific Personal Values for Task 1\n",
      " - Number, k, to use in k-NN:      3\n",
      " - Value of p for the norm ||.||p: 3\n",
      "Items in the target columns:  ['Benign' 'Malignant']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>compactness error</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>texture error</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>0.04904</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.9053</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>0.01308</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.7339</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>0.04006</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.7869</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>0.07458</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>1.1560</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>0.02461</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.7813</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  compactness error  worst concave points  texture error  \\\n",
       "0        17.99            0.04904                0.2654         0.9053   \n",
       "1        20.57            0.01308                0.1860         0.7339   \n",
       "2        19.69            0.04006                0.2430         0.7869   \n",
       "3        11.42            0.07458                0.2575         1.1560   \n",
       "4        20.29            0.02461                0.1625         0.7813   \n",
       "\n",
       "   worst fractal dimension  target  \n",
       "0                  0.11890  Benign  \n",
       "1                  0.08902  Benign  \n",
       "2                  0.08758  Benign  \n",
       "3                  0.17300  Benign  \n",
       "4                  0.07678  Benign  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Create a data frame, using the feature data as column headings\n",
    "dfbigbc = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "# Add a target column at the end, and fill it with the target data\n",
    "dfbigbc['target'] = data.target   # target 0/1 means malignant/benign\n",
    "\n",
    "# Make the 1/0 more user friendly: taken from (24 feb 2023)\n",
    "# https://www.datacamp.com/tutorial/principal-component-analysis-in-python\n",
    "dfbigbc['target'].replace(0, 'Benign', inplace=True)\n",
    "dfbigbc['target'].replace(1, 'Malignant', inplace=True)\n",
    "\n",
    "# This dataset has a lot of features - we'll work with a subset\n",
    "print('Number of Original Features: ', len(data.feature_names))\n",
    "print('Original data Frame shape: dfbigbc.shape = ', dfbigbc.shape)\n",
    "# set a random seed dependent on the student ID\n",
    "random.seed(ID+30)\n",
    "# get a list of integers indexing feature columns 0,1,2,...,29\n",
    "nums = list(range(0,30))\n",
    "# shuffle them randomly and add the target column index on at the end\n",
    "random.shuffle(nums)\n",
    "newnums = nums[0:5]\n",
    "newnums.append(30)\n",
    "print(f'We will work only with the features in columns: {newnums}')\n",
    "dfbc = dfbigbc.iloc[:,newnums]\n",
    "print('These features are ...')\n",
    "print(list(dfbc))\n",
    "# get personalized algorithm parameters\n",
    "kn = random.randint(3, 8)\n",
    "pn = random.randint(1, 10)\n",
    "print('Specific Personal Values for Task 1')\n",
    "print(f' - Number, k, to use in k-NN:      {kn}')\n",
    "print(f' - Value of p for the norm ||.||p: {pn}')\n",
    "print('Items in the target columns: ', dfbc.target.unique())\n",
    "dfbc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have access to a data frame `dfbc`, and above you will find\n",
    "your values of $k$ and $p$.\n",
    "\n",
    "**- - SUBMIT YOUR WORK FOR TASK 1 IN THE CELL(S) BELOW - -**\n",
    "\n",
    "**- - CREATE MORE CELLS AS NEEDED - -**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 2\n",
    "\n",
    "This task is a continuation of Task 1 and involves PCA (*Principal Component Analysis*).\n",
    "You should use the data subset and personalized values from above. Also, after executing\n",
    "the *untouchable* code below you will get a personalized value of `nc`. This is the number\n",
    "of principal component you should use. \n",
    "\n",
    "In this notebook for Task 2\n",
    "\n",
    "- Use PCA to analyze the variance in your training data. You may use\n",
    "`sklearn` for this or work from basic principles.\n",
    "- How many principal components are there?\n",
    "- Produce a plot or bar graph of the explained variance percentages for all components.\n",
    "- Perform PCA to compress your training data using `nc` components. You may use\n",
    "`sklearn` for this or work from basic principles.\n",
    "- Produce a plot or bar graph of the explained variance percentages the `nc` component(s).\n",
    "- Re-run the $k$-NN method using the data compression resulting from choosing\n",
    "just `nc` principal components.\n",
    "- Obtain the accuracy score and a confusion matrix as above\n",
    "- *You must use the same training and test data as above*\n",
    "- *You must adhere to the principal that the __test data is regarded as unseen__**\n",
    "\n",
    "In you report for Task 2\n",
    "\n",
    "- Give a short overview of PCA. Include main concepts and formulae as necessary but\n",
    "do not give proofs or derivations.\n",
    "- Explain how much variance is captured by your value of `nc`.\n",
    "- Discuss the results in terms of accuracies and confusion matrices. Are they comparable?\n",
    "Do you recommend the use of just `nc` principal components for this model? Feel free to use probabilistic\n",
    "arguments to elicit the advantages and disadvantages.\n",
    "- Don't spell *principal* as *principle*. This will be an unconditional fail and you\n",
    "will be asked to leave the Earth for ever. (Just Kidding!)\n",
    "\n",
    "\n",
    "**- - DO NOT ALTER THE CONTENTS OF THE NEXT CELL(S) IN ANY WAY - -**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You should use 1 principal components for you data compression\n",
      "The variable nc should be used for this\n"
     ]
    }
   ],
   "source": [
    "nc = random.randint(1, 4)\n",
    "print(f'You should use {nc} principal components for you data compression')\n",
    "print('The variable nc should be used for this')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing the untouchable cell above you will see how many \n",
    "principal components - nc - you should use in your analysis below.\n",
    "\n",
    "**- - SUBMIT YOUR WORK FOR TASK 2 IN THE CELL(S) BELOW - -**\n",
    "\n",
    "**- - CREATE MORE CELLS AS NEEDED - -**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 3\n",
    "\n",
    "You will compress daily stock data by performing a\n",
    "*Singular Value Decomposition* (SVD). \n",
    "You will use the SVD transformation to add\n",
    "additional data and illustrate the augmented data set graphically.\n",
    "\n",
    "The untouchable code below will set up the dataframes for you but you will need to\n",
    "obtain the CSV files from Brightspace. They are called `TSLAhistory.csv`\n",
    "and `TSLAupdate.csv`.\n",
    "\n",
    "Once you have executed the code below you will have access to two dataframes.\n",
    "**This code should not be altered**.\n",
    "\n",
    "The data frame in `dfth` will contain historical data for the TESLA share price. \n",
    "A set of more recently aquired data is in `dftu`. The real-world situation\n",
    "we are simulating here is that you have an intial download of data, and you have\n",
    "performed an SVD on it so that you can select the dominant transformed components\n",
    "and use those as a **training set** for your machine learning tools. An updated set\n",
    "of data arrives. These data points are **unseen** as far as your analysis tools are\n",
    "concerned and so can be designated as a test set. However, your codes have been\n",
    "trained on SVD-transformed data and so the test set needs also to be transformed\n",
    "to be comptatible.\n",
    "\n",
    "\n",
    "In this notebook for Task 3\n",
    "\n",
    "- Run the untouchable code. Check the data is *clean*. If it isn't then clean it up.\n",
    "- Use *seaborn* and `sns.pairplot` to create a pair plot for `dfth`. \n",
    "- Produce a combined scatter plot of *Volume* vertically against *Open* horizontally with both data sets but distinguished by colour.\n",
    "- Select training data, `X_train`, from `dfth` using all columns except *Date* and *Adj Close*. \n",
    "- Perform an SVD of this training data and determine the rank of the data set.\n",
    "- Create a (logarithmic) scree plot from the singular values. \n",
    "- Create `Xc_train`, an SVD-compressed version of the training data formed by taking just the first `c` dominant singular components.\n",
    "- Use `linalg.norm(X_train - Xc_train)` from `numpy` to calculate the error in the SVD approximation of `X_train` by `Xc_train`. Plot a graph, or bar chart, of this error against all appropriate values of $c$. \n",
    "- Create a compressed training data set using $c=1$ by SVD transformation of `X_train` to a transformed training set, called, for example, `Kc`.\n",
    "- Create a scatter plot of *open* against *Volume* with `X_train` and `X_test` on the same set of axes, but in different colours. Make sure that your axes are labelled correctly. You can use, for example,\n",
    "\n",
    "```\n",
    "# put both of these in the same cell \n",
    "plt.scatter(X_train[:,0], X_train[:,4], color='red')\n",
    "plt.scatter(X_test[:,0],  X_test[:,4],  color='blue')\n",
    "plt.xlabel('Open'); plt.ylabel('Volume')\n",
    "```\n",
    "\n",
    "- Now create similar scatter plot but with `Kc` and `X_test`. How does this plot differ from the last? Explain this difference.\n",
    "- Transform the test data `X_test` to, say, `Qc`.\n",
    "- Create yet another similar scatter plot but with `Kc` and `Qc`.\n",
    "- Repeat the construction of these three scatter plots but with $c=2$. Comment on the results. In particular contrast and compare these plots to the $c=1$ plots.\n",
    "\n",
    "\n",
    "In you report for Task 3\n",
    "\n",
    "- Give a short overview of the *Singular Value Decomposition* (SVD).\n",
    "- Refer to your pairplot - discuss its features. Thinking ahead, how many dominant independent components would you expect to lie in these data?\n",
    "- Give the rank of the data set.\n",
    "- Give an outline of the mathematical details of your SVD-transformation of `X_train` to `Kc`.\n",
    "- Give an outline of the mathematical details and a justification for your method of transformation of `X_test` to `Qc`.\n",
    "- For $c=1$, how does the second scatter plot differ from the first? Explain this difference. \n",
    "- For $c=1$, how does the third scatter plot differ from the first two? Explain this difference. \n",
    "- For $c=2$, how do these plots change? How do you interpret this change?\n",
    "\n",
    "\n",
    "**- - DO NOT ALTER THE CONTENTS OF THE NEXT CELL IN ANY WAY - -**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfth = pd.read_csv(\"TSLAhistory.csv\")\n",
    "dftu = pd.read_csv(\"TSLAupdate.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing the untouchable cell above you will see how many \n",
    "principal components - nc - you should use in your analysis below.\n",
    "\n",
    "**- - SUBMIT YOUR WORK FOR TASK 3 IN THE CELL(S) BELOW - -**\n",
    "\n",
    "**- - CREATE MORE CELLS AS NEEDED - -**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Notebook\n"
     ]
    }
   ],
   "source": [
    "print('End of Notebook')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
